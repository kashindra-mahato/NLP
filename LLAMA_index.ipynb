{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kashindra-mahato/NLP/blob/main/LLAMA_index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLAMA index\n",
        "- llama index is a simple, flexible interface between external data and llms.\n",
        "- We have used llama index for indexing our raw documents(in txt format), and query the index.\n",
        "- We are using gpt 3.5 turbo model of openai for indexing and querying.\n",
        "- Two modes namely compact, and QA are used here to demonstrate the effectiveness of gpt with llama indexing."
      ],
      "metadata": {
        "id": "h6mBriI-o6Ap"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbeOL4yC3IEJ"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjaANYi2PAU6",
        "outputId": "08c6e806-dab9-4704-e77d-566eb41a4d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "EUHCm0X2Pc2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "Irik3o3UctIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "vxs2dDBGO-8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n"
      ],
      "metadata": {
        "id": "ijznBd67PE-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import (\n",
        "    LLMPredictor,\n",
        "    GPTVectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    MockLLMPredictor,\n",
        "    MockEmbedding,\n",
        "    ServiceContext,\n",
        "    StorageContext,\n",
        "    PromptHelper,\n",
        "    load_index_from_storage,\n",
        "    ResponseSynthesizer,\n",
        "    QuestionAnswerPrompt\n",
        "    )\n",
        "from langchain import OpenAI\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "from llama_index.retrievers import VectorIndexRetriever\n",
        "from llama_index.indices.postprocessor import (\n",
        "    SimilarityPostprocessor,\n",
        "    AutoPrevNextNodePostprocessor,\n",
        "    PrevNextNodePostprocessor\n",
        "    )\n",
        "from llama_index.storage.docstore import SimpleDocumentStore\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "14y-Cfpr-eLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexing\n",
        "- We define the model used for indexing and quering in LLMPredictor.\n",
        "- PromptHelper helps to fill in the prompt, split the text, and fill in context information. Here we have used PromptHelper to define the parameters of openai model.\n",
        "- ServiceContext is used for configuration, such as LLMPredictor(for configuring the LLM), the PromptHelper(for configuring input size/chunk size), the BaseEmbedding (for configuring embedding model).\n",
        "- For index document store we have two options, we can either use Simple index store(SimpleIndexStore) or Vector stores. Here we have used a vector store(GPTVectorStoreIndex).\n",
        "- We also have two options to use while indexing, either by diving the documents into nodes manually and then indexing those nodes or directly indexing the document(as we have done below). Indexing the document directly will divide the documents into nodes automatically.\n",
        "- Then we are using persist method of storage_context, to save the index. It will save the index in as index_store.json, vector_store.json and docstore.json."
      ],
      "metadata": {
        "id": "yEz6_FaDdWiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_index(data_directory):\n",
        "  # llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))\n",
        "  llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
        "  max_input_size = 4090\n",
        "  num_output = 256\n",
        "  max_chunk_overlap = 20\n",
        "  chunk_size_limit=256\n",
        "  prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
        "  service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
        "  documents = SimpleDirectoryReader(data_directory).load_data()\n",
        "  service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
        "  index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "  index.storage_context.persist(persist_dir=data_directory+\"\")"
      ],
      "metadata": {
        "id": "aqBm-3YV-Ey7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model selection\n",
        "- text-davinci-003 is a text generation model, which takes a prompt as input and responses the text as instructed.\n",
        "- similarly gpt-3.5-turbo is a chat model, which has role specified as system, user and assistant, we define behavior of chatbot under system, and provide examples of chatbot interaction under user and assistant.\n",
        "- text-davinci-003 cost 10 times more than gpt-3.5-turbo, therefore our choice of model is gpt-3.5-turbo. It can also act as retrieval model similar to text-davinci-003.\n",
        "- We need to import gpt-3.5-turbo from langchain.chat_models as ChatOpenAI\n"
      ],
      "metadata": {
        "id": "7G_2SEUhA0YF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the index\n",
        "- To load the index from storage we have to use same ServiceContext used while indexing."
      ],
      "metadata": {
        "id": "leXr7bpYCnZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_index(indexed_dir_path):\n",
        "  llm_predictor = LLMPredictor()\n",
        "  service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
        "  storage_context = StorageContext.from_defaults(persist_dir=indexed_dir_path)\n",
        "  index = load_index_from_storage(storage_context=storage_context, service_context=service_context)\n",
        "  return index"
      ],
      "metadata": {
        "id": "MGW_h0wNLrJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents_path = \"\""
      ],
      "metadata": {
        "id": "Q12HnPkoOTFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_index(documents_path)"
      ],
      "metadata": {
        "id": "9LZGcp2zOkIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token Count"
      ],
      "metadata": {
        "id": "Ft2WHMRpdaqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- LlamaIndex offers token predictors to predict token usage of LLM and embedding calls.\n",
        "- We are using MockLLMPredictor to predict the token usage during the index querying.\n",
        "- We are using MockEmbedding in tandem with MockPredictor for token usage of embedding calls."
      ],
      "metadata": {
        "id": "OLuOIzLd-8us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_indexing_token_count(data_directory):\n",
        "  llm_predictor = MockLLMPredictor(max_tokens=256)\n",
        "  embed_model = MockEmbedding(embed_dim=1536)\n",
        "  service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model)\n",
        "  documents = SimpleDirectoryReader(data_directory).load_data()\n",
        "  index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "  return llm_predictor.last_token_usage"
      ],
      "metadata": {
        "id": "WcIGjvUNgQhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexing_count = get_indexing_token_count(documents_path)"
      ],
      "metadata": {
        "id": "HboFK_z8hVhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexing_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEk9Qe3qhcUb",
        "outputId": "d5113d51-de91-46b0-bc9c-37fef33d9b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### token count for compact method\n",
        "- the compact mode will \"compact\" the promt during each LLM call by stuffing as many chunks to stuff in one prompt. Then \"create and refine\"(which is the default mode) an answer by going through multiple prompts.\n"
      ],
      "metadata": {
        "id": "LcITBE9rdjb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexed = load_index(documents_path+\"\")\n",
        "def get_token_count_c(query):\n",
        "  index = indexed\n",
        "  llm_predictor = MockLLMPredictor(max_tokens=256)\n",
        "  embed_model = MockEmbedding(embed_dim=1536)\n",
        "  service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model)\n",
        "  retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=2,\n",
        "  )\n",
        "  response_synthesizer = ResponseSynthesizer.from_args(\n",
        "    node_postprocessors=[\n",
        "        SimilarityPostprocessor(similarity_cutoff=0.7)\n",
        "    ]\n",
        "  )\n",
        "  query_engine = RetrieverQueryEngine.from_args(retriever=retriever, response_synthesizer=response_synthesizer, service_context=service_context, response_mode='compact')\n",
        "  response = query_engine.query(query)\n",
        "  return response, llm_predictor.last_token_usage"
      ],
      "metadata": {
        "id": "RMRwotPektN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response, count = get_token_count_c(\"\")"
      ],
      "metadata": {
        "id": "CpAS8YLlk66O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.response"
      ],
      "metadata": {
        "id": "JqHbQHCLvVcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5lAIyvMlQMZ",
        "outputId": "0360c392-f2b4-44c8-9a46-ea6cf69d7ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1391"
            ]
          },
          "metadata": {},
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token count for QA method\n",
        "- A QuestionAnswerPompt is defined to answer the user queries based on the documents provided, and limiting the answer to the predefined number of words."
      ],
      "metadata": {
        "id": "LF015T8JdpsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexed = load_index(documents_path+\"\")\n",
        "def get_token_count_qa(query):\n",
        "  index = indexed\n",
        "  llm_predictor = MockLLMPredictor(max_tokens=256)\n",
        "  embed_model = MockEmbedding(embed_dim=1536)\n",
        "  service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model)\n",
        "  QA_PROMPT_TMPL = (\n",
        "      \"We have provided context information below. \\n\"\n",
        "      \"---------------------\\n\"\n",
        "      \"{context_str}\"\n",
        "      \"\\n---------------------\\n\"\n",
        "      \"Given this information, please answer the question in no more than 50 words: {query_str}\\n\"\n",
        "  )\n",
        "  QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)\n",
        "  retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=2,\n",
        "  )\n",
        "  response_synthesizer = ResponseSynthesizer.from_args(\n",
        "    node_postprocessors=[\n",
        "        SimilarityPostprocessor(similarity_cutoff=0.7)\n",
        "    ]\n",
        "  )\n",
        "  query_engine = RetrieverQueryEngine.from_args(retriever=retriever, response_synthesizer=response_synthesizer, service_context=service_context, text_qa_template=QA_PROMPT)\n",
        "  response = query_engine.query(query)\n",
        "  return response, llm_predictor.last_token_usage"
      ],
      "metadata": {
        "id": "7dKu9MbFYFfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response, count = get_token_count_qa(\"\")"
      ],
      "metadata": {
        "id": "w0uR8m5EOoqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV_M894aYQlj",
        "outputId": "b2b4735d-ef46-4a44-e01f-1f0cf694a8d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2086"
            ]
          },
          "metadata": {},
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrievers, Response Synthesizer and Query Engine\n",
        "- In both compact and QA methods, we have used a retriever, a response synthesizer and a query engine.\n",
        "- A retriever class retrieves a set of Nodes from an index given a query. We can specify a value of similarity_top_k which is the number top nodes to retrieve.\n",
        "- A response synthesizer class takes in a set of Nodes and synthesizes an answer given a query. We can specify node_postprocessors, which is a list of post processors that can further enhance the quality of response generated.\n",
        "- A query engine class takes in a query and returns a response object. It make use of retriever and response synthesizer modules under the hood.\n"
      ],
      "metadata": {
        "id": "9_myVdSDyhDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get answer method for 'compact'"
      ],
      "metadata": {
        "id": "P8FKR-hddwsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexed = load_index(documents_path+\"\")\n",
        "def get_answer(query):\n",
        "  index = indexed\n",
        "  retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=2,\n",
        "  )\n",
        "  response_synthesizer = ResponseSynthesizer.from_args(\n",
        "    node_postprocessors=[\n",
        "        SimilarityPostprocessor(similarity_cutoff=0.7)\n",
        "    ]\n",
        "  )\n",
        "  query_engine = RetrieverQueryEngine.from_args(retriever=retriever, response_synthesizer=response_synthesizer, response_mode='compact')\n",
        "  response = query_engine.query(query)\n",
        "  return response"
      ],
      "metadata": {
        "id": "CiTnF8RHN8rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_answer(\"\")"
      ],
      "metadata": {
        "id": "jGkuSAWOR3Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "44lOoIbHTfn7",
        "outputId": "6a6dc2a6-484e-4ce4-8a0f-06f8a6c69304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe classes available are those related to studying abroad, such as visa compliance, choosing the best course based on preferences, qualification, career goals, and financial circumstances.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get answer method for 'QA'"
      ],
      "metadata": {
        "id": "sGGU6E7Ad4xW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexed = load_index(documents_path+\"\")\n",
        "def get_answer_qa(query):\n",
        "  index = indexed\n",
        "  QA_PROMPT_TMPL = (\n",
        "      \"We have provided context information below. \\n\"\n",
        "      \"---------------------\\n\"\n",
        "      \"{context_str}\"\n",
        "      \"\\n---------------------\\n\"\n",
        "      \"Given this information, please answer the question in less than 50 words: {query_str}\\n\"\n",
        "  )\n",
        "  QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)\n",
        "  retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=2,\n",
        "  )\n",
        "  response_synthesizer = ResponseSynthesizer.from_args(\n",
        "    node_postprocessors=[\n",
        "        SimilarityPostprocessor(similarity_cutoff=0.7)\n",
        "    ]\n",
        "  )\n",
        "  query_engine = RetrieverQueryEngine.from_args(retriever=retriever, response_synthesizer=response_synthesizer, text_qa_template=QA_PROMPT)\n",
        "  response = query_engine.query(query)\n",
        "  return response\n"
      ],
      "metadata": {
        "id": "rf2xvgykAXvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_answer_qa(\"\")"
      ],
      "metadata": {
        "id": "fxNHy7b9Jsuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cZ6I1vcwJtZu",
        "outputId": "750bef2c-ed1a-4f7c-fd17-748b8cb7d80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTest preparation classes available include online classes, classes with qualified teachers and mentors, and practice exams.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 271
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test case\n",
        "- We have a total of 544 queries in test case.\n",
        "- We are getting the token count and response from both 'compact' and 'QA prompt' methods and appending the result to the existing csv file along with query time for each question."
      ],
      "metadata": {
        "id": "ARNl3V24eBE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "NL22XgCmd9Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_df = pd.read_excel('')"
      ],
      "metadata": {
        "id": "88btxcB5KzRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from datetime import datetime\n",
        "def query(test_data, indexed):\n",
        "  for i in range(len(test_data['Queries '])):\n",
        "    query = test_data['Queries '][i]\n",
        "    t1 = datetime.now()\n",
        "    answer_compact = get_answer(query)\n",
        "    t2 = datetime.now()\n",
        "    time_compact = t2-t1\n",
        "    _, token_compact = get_token_count_c(query)\n",
        "    t3 = datetime.now()\n",
        "    answer_qa = get_answer_qa(query)\n",
        "    t4 = datetime.now()\n",
        "    time_qa = t4-t3\n",
        "    _, token_qa = get_token_count_qa(query)\n",
        "    print(\"\\nQuery: \", query)\n",
        "    print(\"\\nAnswer(compact): \", answer_compact)\n",
        "    print(\"\\nAnswer(QA): \", answer_qa)\n",
        "    # Create the dictionary (=row)\n",
        "    row = {'Query':query,'answer_compact':answer_compact,'token_compact':token_compact, 'answer_qa':answer_qa, 'token_qa':token_qa, 'time_compact':time_compact, 'time_qa': time_qa}\n",
        "    # Open the CSV file in \"append\" mode\n",
        "    with open('.csv', 'a', newline='') as f:\n",
        "        # Create a dictionary writer with the dict keys as column fieldnames\n",
        "        writer = csv.DictWriter(f, fieldnames=row.keys())\n",
        "        # Append single row to CSV\n",
        "        writer.writerow(row)"
      ],
      "metadata": {
        "id": "y1CtHX6Ld8Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexed = load_index(documents_path+\"\")"
      ],
      "metadata": {
        "id": "LiDRf5LPk9SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query(query_df, indexed)"
      ],
      "metadata": {
        "id": "iLLMyw9gj_NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W0kkKr3hk7nn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMo84nQ9gD4T/ReHBK+y9Q3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}